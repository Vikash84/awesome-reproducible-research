# Awesome Reproducible Research [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3564746.svg)](https://doi.org/10.5281/zenodo.3564746)


<meta property="og:image" content="https://github.com/leipzig/awesome-reproducible-research/blob/master/rrlogo2.png?raw=true"/>
<img src="https://github.com/leipzig/awesome-reproducible-research/blob/master/rrlogo2.png?raw=true" align="right" width="200">

> A curated list of reproducible research case studies, projects, tutorials, and media


## Contents

- [Case studies](#case-studies)
- [Ad-hoc reproductions](#ad-hoc-reproductions)
- [Theory papers](#theory-papers)
- [Tool reviews](#tool-reviews)
- [Courses](#courses)
- [Development Resources](#development-resources)
- [User tools](#user-tools)
- [Books](#books)
- [Databases](#databases)
- [Data Repositories](#data-repositories)
- [Examples and exemplars](#examples-and-exemplars)
- [Journals](#journals)
- [Ontologies](#ontologies)
- [Organizations](#organizations)
- [Awesome Lists](#awesome-lists)

## Case studies
The term "case studies" is used here in a general sense to describe any study of reproducibility. A _reproduction_ is an attempt to arrive at comparable results with identical data using computational methods described in a paper. A _refactor_ involves refactoring existing code into frameworks and other reproducibility best practices while preserving the original data. A _replication_ involves generating new data and applying existing methods to achieve comparable results. A _robustness test_ applies various protocols, workflows, statistical models or parameters to a given data set to study their effect on results, either as a follow-up to an existing study or as a "bake-off". A _census_ is a high-level tabulation conducted by a third party. A _survey_ is a questionnaire sent to practitioners. A _case narrative_ is an in-depth first-person account. An _independent discussion_ utilizes a secondary independent author to interpret the results of a study as a means to improve inferential reproducibility.
<table id="case_studies">
			<tbody>
				<tr>
					<td>
						<p>
							Study <meta property="embeddedDate">
						</p>
					</td>
					<td>
						<p>
							Field
						</p>
					</td>
					<td>
						<p>
							Approach
						</p>
					</td>
					<td>
						<p>
							Size
						</p>
					</td>
					<td>
						<p>
							Result
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1136/bmj.39590.732037.47"><span title="What is missing from descriptions of treatment in trials and reviews?">Glasziou et al <meta property="datePublished" content="2008-06-26">2008</span></a>
						</p>
					</td>
					<td>
						<p>
							Medicine
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							80 studies
						</p>
					</td>
					<td>
	<p>
		<div class="progress">
  <div class="progress-bar" role="progressbar" style="width: 25%;" aria-valuenow="51" aria-valuemin="0" aria-valuemax="100">51%</div>
</div>
						<meta property="result" content="Elements of the intervention were missing in 41 of 80 of the published descriptions"/>
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1214/09-AOAS291"><span title="DERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY">Baggerly &amp; Coombes <meta property="datePublished" content="2009-09-01">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Refactor
						</p>
					</td>
					<td>
						<p>
							8 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1002/bimj.200900154"><span title="Biometrical Journal and Reproducible Research">Hothorn et al. <meta property="datePublished" content="2009-08-17">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Biostatistics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							56 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/ng.295"><span title="Repeatability of published microarray gene expression analyses">Ioannidis et al <meta property="datePublished" content="2009-01-28">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Genetics
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							18 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://www.uio.no/studier/emner/matnat/ifi/INF5700/h11/undervisningsmateriale/Anda.Sj%C3%B8berg.Mockus.TSE.May.2009.pdf">Anda et al <meta property="datePublished" content="2009-07-30">2009</a>
						</p>
					</td>
					<td>
						<p>
							Software engineering
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							4 companies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://infoscience.epfl.ch/record/136640/files/VandewalleKV09.pdf">Vandewalle et al <meta property="datePublished" content="2009-04-22">2009</a>
						</p>
					</td>
					<td>
						<p>
							Signal processing
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							134 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/nrd3439-c1">Prinz <meta property="datePublished" content="2011-08-31">2011</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							23 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://academic.oup.com/bib/article/12/3/288/258098/Case-studies-in-reproducibility">Horthorn &amp; Leisch <meta property="datePublished" content="2011-01-28">2011</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							100 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.nature.com/nature/journal/v483/n7391/full/483531a.html">Begley &amp; Ellis <meta property="datePublished" content="2012-03-29">2012</a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							53 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://reproducibility.cs.arizona.edu/tr.pdf">Collberg et al <meta property="datePublished" content="2014-03-21">2014</a><br/><a href="https://sci-hub.tw/10.1145/2812803">Collberg &amp; Proebsting 2016</a>
						</p>
					</td>
					<td>
						<p>
							Computer science
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							613 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://science.sciencemag.org/content/349/6251/aac4716">OSC <meta property="datePublished" content="2015-08-28">2015</a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							100 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
					<p>
					<a href="https://f1000research.com/articles/4-134/v2">Bandrowski et al <meta property="datePublished" content="2015-05-29">2015</a>
					</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							100 papers
						</p>
					</td>
				</tr>
								<tr>
					<td>
					<p>
					<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4555355/">Patel et al <meta property="datePublished" content="2015-06-06">2015</a>
					</p>
					</td>
					<td>
						<p>
							Epidemiology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							417 variables
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.17016/FEDS.2015.083">Chang et al
							<meta property="datePublished" content="2015-10-05">2015</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say &quot;Usually Not&quot;">Economics</span>
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							67 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.1002333">Iqbal et al
							<meta property="datePublished" content="2016-01-04">2016</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible Research Practices and Transparency across the Biomedical Literature">Biomedical sciences</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							441 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
					<p>
					<a href="https://doi.org/10.1038/533452a"><span title="1,500 scientists lift the lid on reproducibility">Baker <meta property="datePublished" content="2016-05-26">2016</span></a>
					</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							<span title="More than 70% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature's survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research.">Survey</span>
						</p>
					</td>
					<td>
						<p>
							1,576 researchers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://pdfs.semanticscholar.org/edd7/e68711955cbbdb6dd6866db2ec8a6ff18278.pdf">Névéol et al <meta property="datePublished" content="2016-11-05">2016</a>
						</p>
					</td>
					<td>
						<p>
							NLP
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							3 studies
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<a href="https://elifesciences.org/articles/23383#abstract">Reproducibility Project <meta property="datePublished" content="2017-01-19">2017</a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							9 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
						<a href="https://peerj.com/articles/3208/">Vasilevsky et al <meta property="datePublished" content="2017-04-25">2017</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							318 journals
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
						<a href="https://www.practicereproducibleresearch.org/">Kitzes et al <meta property="datePublished" content="2017-10-17">2017</a>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							31 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005755">Barone et al <meta property="datePublished" content="2017-10-19">2017</a>
						</p>
					</td>
					<td>
						<p>
						        Biological sciences
						<p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							704 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.biorxiv.org/content/biorxiv/early/2017/10/31/143503.full.pdf?%3Fcollection=">Kim &amp; Dumas <meta property="datePublished" content="2017-10-31">2017</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Refactor
						</p>
					</td>
					<td>
						<p>
							1 study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://science.sciencemag.org/content/351/6280/1433">Camerer <meta property="datePublished" content="2016-03-25">2017</a>
						</p>
					</td>
					<td>
						<p>
							Economics
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							18 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://openreview.net/pdf?id=By4l2PbQ-">Olorisade <meta property="datePublished" content="2017-08-06">2017</a>
						</p>
					</td>
					<td>
						<p>
							Machine learning
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							30 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1515/opar-2017-0019">Strupler &amp; Wilkinson <meta property="datePublished" content="2017-11-14">2017</a>
						</p>
					</td>
					<td>
						<p>
							Archaeology
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 survey
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/1801.05042">Danchev et al <meta property="datePublished" content="2018-01-15">2017</a>
						</p>
					</td>
					<td>
						<p>
							Comparative toxicogenomics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							51,292 claims in 3,363 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.researchgate.net/publication/326450530_State_of_the_Art_Reproducibility_in_Artificial_Intelligence">Kjensmo &amp; Gundersen <meta property="datePublished" content="2018-04-25">2018</a>
						</p>
					</td>
					<td>
						<p>
							Artificial intelligence
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							400 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.nature.com/articles/d41586-018-02108-9">Gertler et al <meta property="datePublished" content="2018-02-21">2018</a>
						</p>
					</td>
					<td>
						<p>
							Economics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							203 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://www.pnas.org/content/115/11/2584">Stodden et al <meta property="datePublished" content="2018-03-13">2018</a>
						</p>
					</td>
					<td>
						<p>
							Computational science
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							204 articles, 180 authors
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://doi.org/10.1371/journal.pone.0213013">Madduri et al <meta property="datePublished" content="2019-04-09">2018</a>
						</p>
					</td>
					<td>
						<p>
							Genomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/s41562-018-0399-z">Camerer et al <meta property="datePublished" content="2018-08-27">2018</a>
						</p>
					</td>
					<td>
						<p>
							Social sciences
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							21 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://doi.org/10.1177/2515245917747646">Silberzahn et al <meta property="datePublished" content="2018-08-23">2018</a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							One data set, 29 analyst teams
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1002/bimj.201700243">Boulesteix et al <meta property="datePublished" content="2018-08-01">2018</a>
						</p>
					</td>
					<td>
						<p>
							Medicine and health sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							30 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.7554/eLife.34364">Eaton et al <meta property="datePublished" content="2018-10-03">2018</a>
						</p>
					</td>
					<td>
						<p>
							Microbiome immuno oncology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.biorxiv.org/content/early/2018/11/08/463927">Vaquero-Garcia et al <meta property="datePublished" content="2018-11-08">2018</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Refactor and test of robustness
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.2006930">Wallach et al <meta property="datePublished" content="2018-11-20">2018</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical Sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							149 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://broad.io/ASHG2018">Miller et al <meta property="datePublished" content="2018-10-18">2018</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Synthetic replication & refactor
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1080/13658816.2018.1508687">Konkol et al <meta property="datePublished" content="2018-04-09">2018</a>
						</p>
					</td>
					<td>
						<p>
							Geosciences
						</p>
					</td>
					<td>
						<p>
							Survey, Reproduction
						</p>
					</td>
					<td>
						<p>
							146 scientists, 41 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://amid.fish/reproducing-deep-rl">Rahtz <meta property="datePublished" content="2018-04-06">2018<a>
								</p>
								</td>
							<td>
								<p>
									Reinforcement Learning
								</p>
							</td>
							<td>
								<p>
									Reproduction, case narrative
								</p>
							<td>
								<p>
									1 paper
								</p>
							</td>
						</p>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1145/3214239.3214242">Stodden et al
							<meta property="datePublished" content="2018-06-12">2018</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Enabling the Verification of Computational Results">Computational physics</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							306 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://peerj.com/articles/cs-163/">AlNoamany & Borghi <meta property="datePublished" content="2018-09-17">2018</a>
						</p>
					</td>
					<td>
						<p>
							Science & Engineering
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							215 participants
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1155/2018/4789035
">Li et al <meta property="datePublished" content="2018-09-27">2018</a>
						</p>
					</td>
					<td>
						<p>
							Nephrology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://dash.harvard.edu/bitstream/handle/1/38811561/CHEN-SENIORTHESIS-2018.pdf?sequence=3
">Chen <meta property="datePublished" content="2018-06-29">2018</a>
						</p>
					</td>
					<td>
						<p>
							Social sciences & other
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							810 Dataverse studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.7717/peerj.5072">Nüst et al
							<meta property="datePublished" content="2018-07-13">2018</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible research and GIScience: an evaluation using AGILE conference papers">GIScience/Geoinformatics</span>
						</p>
					</td>
					<td>
						<p>
							Census, Survey
						</p>
					</td>
					<td>
						<p>
							32 papers, 22 participants
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/sdata.2019.30">Stagge et al <meta property="datePublished" content="2019-02-26">2019</a>
						</p>
					</td>
					<td>
						<p>
							Geosciences
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							360 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1006269">Bizzego et al <meta property="datePublished" content="2019-03-27">2019</a>
						</p>
					</td>
					<td>
						<p>
							Deep learning
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 analysis
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0213013">Madduri et al <meta property="datePublished" content="2019-04-11">2019</a>
						</p>
					</td>
					<td>
						<p>
							Genomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 analysis
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<span title="Creating reproducible pharmacogenomic analysis pipelines"><a href="10.1038/s41597-019-0174-7">Mammoliti
								et al <meta property="datePublished" content="2019-09-03">2019</a></span>
						</p>
					</td>
					<td>
						<p>
							Pharmacogenomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							2 analyses
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1371/journal.pbio.3000246"><span title="Open science challenges, benefits and tips in early career and beyond">Allen & Mehler <meta property="datePublished" content="2019-05-01">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences and Psychology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							127 registered reports 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1109/MSR.2019.00077"><span title="A Large-scale Study about Quality and
Reproducibility of Jupyter Notebooks">Pimentel et al <meta property="datePublished" content="2019-05-07">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							All
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							 1,159,166 Jupyter notebooks 
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.omto.2019.05.004"><span title="Assessing the Completeness of Reporting in Preclinical Oncolytic Virus Therapy Studies">Fergusson et al <meta property="datePublished" content="2019-05-20">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Virology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							 236 papers 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.bja.2019.01.012
"><span title="Hypnotic depth and postoperative death: a Bayesian perspective and an Independent Discussion of a clinical trial">Vlisides et al <meta property="datePublished" content="2019-01-22">2019</span></a><br/><a href="https://doi.org/10.1016/j.bja.2018.12.021"><span title="Depth of sedation as an interventional target to reduce postoperative delirium: mortality and functional outcomes of the Strategy to Reduce the Incidence of Postoperative Delirium in Elderly Patients randomised clinical trial">Sieber et al 2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Anaesthesia
						</p>
					</td>
					<td>
						<p>
							Indepedent discussion
						</p>
					</td>
					<td>
						<p>
							 1 study 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://osf.io/7yt8u/
"><span title="Replication Oxley et al. (2008, Science)">Bakker et al <meta property="datePublished" content="2019-06-19">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							 1 paper 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.cels.2019.06.005
"><span title="A Multi-center Study on the Reproducibility of Drug-Response Assays in Mammalian Cell Lines">Niepel et al <meta property="datePublished" content="2019-07-10">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Cell pharmacology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							 5 labs 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://arxiv.org/abs/1907.06902v1"><span title="
Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches">Dacrema et al <meta property="datePublished" content="2019-07-16">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Machine learning
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							18 conference papers 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.jasrep.2019.102002"><span title="Experimental replication shows knives manufactured from frozen human feces do not work">Eran et al <meta property="datePublished" content="2019-10-09">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Experimental archaeology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							1 theory
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/763730"><span title="Reproducible and Transparent Research Practices in Published Neurology Research">Rauh et al <meta property="datePublished" content="2019-09-16">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Neurology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							202 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/dkg53"><span title="Failed pre-registered replication of mortality salience effects in traditional and novel measures">Sætrevik & Sjåstad <meta property="datePublished" content="2019-09-20">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							2 experiments
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1038/s41559-019-0972-5"><span title="A checklist for maximizing reproducibility of ecological niche models">Feng et al. <meta property="datePublished" content="2019-09-23">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Ecology and Evolution
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							163 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/843193"><span title="Variability in the analysis of a single neuroimaging dataset by many teams">Botvinik-Nezer et al. <meta property="datePublished" content="2019-11-15">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Neuroimaging
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 data set, 70 teams
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/vef2c"><span title="Many Labs 4: Failure to Replicate Mortality Salience Effect With and Without Original Author Involvement">Klein et al. <meta property="datePublished" content="2019-12-11">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							<span title="Many Labs 4: Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 21 Labs and N = 2,220 participants) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not yet understood or no longer exist.">Replication</span>
						</p>
					</td>
					<td>
						<p>
							1 experiment, 21 labs, 2,220 participants
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/fk8vh"><span title="Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology">Obels et al. <meta property="datePublished" content="2019-05-23">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							<span title="Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to re-use or check published research. However, these benefits will only emerge if researchers can reproduce the analysis reported in published articles and if data is annotated well enough so that it is clear what all variables mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify practices that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature between 2014 and 2018, and attempted to independently computationally reproduce the main results in each article. Of the main results from 62 articles that met our inclusion criteria, data were available for 41 articles, and analysis scripts for 37 articles. For the main results in 36 articles that shared both data and code we could run the scripts for 31 analyses, and reproduce the main results for 21 articles. Although the articles that shared both data and code (36 out of 62, or 58%) and articles for which main results could be computationally reproduced (21 out of 36, or 58%) was relatively high compared to other studies, there is clear room for improvement. We provide practical recommendations based on our observations and link to examples of good research practices in the papers we reproduced.">Census</span>
						</p>
					</td>
					<td>
						<p>
							62 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1001/jamaoncol.2019.2564">Wayant et al
							<meta property="datePublished" content="2019-09-05">2019</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Evaluation of Reproducible Research Practices in Oncology Systematic Reviews With Meta-analyses Referenced by National Comprehensive Cancer Network Guidelines">Oncology</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							154 meta-analyses
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/2020.01.30.924092"><span title="Factorial study of the RNA-seq computational workflow identifies biases as technical gene signatures">Simoneau et al. <meta property="datePublished" content="2020-01-30">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							<span title="RNA-seq is a modular experimental and computational approach that aims in identifying and quantifying RNA molecules. The modularity of the RNA-seq technology enables adaptation of the protocol to develop new ways to explore RNA biology, but this modularity also brings forth the importance of methodological thoroughness. Liberty of approach comes with the responsibility of choices, and such choices must be informed. Here, we present an approach that identifies gene group specific quantification biases in currently used RNA-seq software and references by processing sequenced datasets using a wide variety of RNA-seq computational pipelined, and by decomposing these expression datasets using an independent component analysis matrix factorisation method. By exploring the RNA-seq pipeline using a systemic approach, we highlight the yet inadequately characterized central importance of genome annotations in quantification results. We also show that the different choices in RNA-seq methodology are not independent, through interactions between genome annotations and quantification software. Genes were mainly found to be affected by differences in their sequence, by overlapping genes and genes with similar sequence. Our approach offers an explanation for the observed biases by identifying the common features used differently by the software and references, therefore providing leads for the betterment of RNA-seq methodology.">Robustness test</span>
						</p>
					</td>
					<td>
						<p>
							1 data set
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1186/s13041-020-0552-2"><span title="No raw data, no science: another possible source of the reproducibility crisis">Miyakawa <meta property="datePublished" content="2020-02-20">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							Neurobiology
						</p>
					</td>
					<td>
						<p>
							<span title="A reproducibility crisis is a situation where many scientific studies cannot be reproduced. Inappropriate practices of science, such as HARKing, p-hacking, and selective reporting of positive results, have been suggested as causes of irreproducibility. In this editorial, I propose that a lack of raw data or data fabrication is another possible cause of irreproducibility. As an Editor-in-Chief of Molecular Brain, I have handled 180 manuscripts since early 2017 and have made 41 editorial decisions categorized as Revise before review, requesting that the authors provide raw data. Surprisingly, among those 41 manuscripts, 21 were withdrawn without providing raw data, indicating that requiring raw data drove away more than half of the manuscripts. I rejected 19 out of the remaining 20 manuscripts because of insufficient raw data. Thus, more than 97% of the 41 manuscripts did not present the raw data supporting their results when requested by an editor, suggesting a possibility that the raw data did not exist from the beginning, at least in some portions of these cases. Considering that any scientific study should be based on raw data, and that data storage space should no longer be a challenge, journals, in principle, should try to have their authors publicize raw data in a public database or journal site upon the publication of the paper to increase reproducibility of the published results and to increase public trust in science.">Census</span>
						</p>
					</td>
					<td>
						<p>
							41 papers
						</p>
					</td>
				</tr><tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0229578">Thelwall et al
							<meta property="datePublished" content="2020-02-21">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Is useful research data usually shared? An investigation of genome-wide association study summary statistics">Genetics</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							1799 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0233107">Maassen et al
							<meta property="datePublished" content="2020-05-27">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducibility of individual effect sizes in meta-analyses in psychology">Psychology</span>
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							33 meta-analyses
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/2020.05.11.088021">Riedel et al
							<meta property="datePublished" content="2020-05-13">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="ODDPub – a Text-Mining Algorithm to Detect Data Sharing in Biomedical Publications">Biomedical science</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							792 papers
						</p>
					</td>
				</tr>
				<!--study_placeholder-->
			</tbody>
		</table>

## Ad-hoc reproductions

These are one-off unpublished attempts to reproduce individual studies

<table id="ad_hoc">
			<tbody>
				<tr>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							Original study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							https://rdoodles.rbind.io/2019/06/reanalyzing-data-from-human-gut-microbiota-from-autism-spectrum-disorder-promote-behavioral-symptoms-in-mice/
							and
							https://notstatschat.rbind.io/2019/06/16/analysing-the-mouse-autism-data/
						</p>
					</td>
					<td>
						<p>
							Sharon, G. et al. Human Gut Microbiota from Autism Spectrum Disorder Promote Behavioral Symptoms in Mice. Cell 2019, 177 (6), 1600–1618.e17.
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							https://github.com/sean-harrison-bristol/CCR5_replication
						</p>
					</td>
					<td>
						<p>
							Wei, X.; Nielsen, R. CCR5-∆32 Is Deleterious in the Homozygous State in Humans. Nat. Med. 2019 DOI: 10.1038/s41591-019-0459-6. (retracted)
						</p>
					</td>
				</tr>
				</tbody>
		</table>
			


## Theory papers
<table>
	<tbody>
				<tr>
					<td>
						<p>
							Authors/Date
						</p>
					</td>
					<td>
						<p>
							Title
						</p>
					</td>
					<td>
						<p>
							Field
						</p>
					</td>
					<td>
						<p>
							Type
						</p>
					</td>
				</tr>
		<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1080/09332480.2019.1579573">Ioannidis <meta property="datePublished" content="2005-08-30">2005</a>
						</p>
					</td>
					<td>
						<p>
							<span title="There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.">Why most published research findings are false</span>
						</p>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Statistical reproducibility
						</p>
			</td>	
			</td>
				</tr>
						<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1000424">Noble <meta property="datePublished" content="2009-07-31">2005</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Most bioinformatics coursework focuses on algorithms, with perhaps some components devoted to learning programming skills and learning how to use existing bioinformatics software. Unfortunately, for students who are preparing for a research career, this type of curriculum fails to address many of the day-to-day organizational challenges associated with performing computational experiments. In practice, the principles behind organizing and documenting computational experiments are often learned on the fly, and this learning is strongly influenced by personal predilections as well as by chance interactions with collaborators or colleagues. The purpose of this article is to describe one good strategy for carrying out computational experiments. I will not describe profound issues such as how to formulate hypotheses, design experiments, or draw conclusions. Rather, I will focus on relatively mundane issues such as organizing files and directories and documenting progress. These issues are important because poor organizational choices can lead to significantly slower research progress. I do not claim that the strategies I outline here are optimal. These are simply the principles and practices that I have developed over 12 years of bioinformatics research, augmented with various suggestions from other researchers with whom I have discussed these issues.">A Quick Guide to Organizing Computational Biology Projects</span>
						</p>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
			</td>	
			</td>
				</tr>
										<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1000424">Sandve et al <meta property="datePublished" content="2013-10-24">2013</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Replication is the cornerstone of a cumulative science [1]. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research [2]. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims [3]. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data [4].">Ten Simple Rules for Reproducible Computational Research</span>
						</p>
					<td>
						<p>
							Computational science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
			</td>	
			</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.1002165"><span title="The Economics of Reproducibility in Preclinical Research">Freedman et al <meta property="datePublished" content="2015-06-09">2015</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Abstract">The Economics of Reproducibility in Preclinical Research</span>
						</p>
					</td>
					<td>
						<p>
							Preclinical research
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.31234/osf.io/jqw35">Yarkoni <meta property="datePublished" content="2019-11-21">2019</a>
						</p>
					</td>
			<td>
						<p>
							<span title="Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned—that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that most inferential statistical tests in psychology fail to meet this basic condition. I demonstrate how foundational assumptions of the 'random effects' model used pervasively in psychology impose far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints dramatically inflates false positive rates and routinely leads researchers to draw sweeping verbal generalizations that lack any meaningful connection to the statistical quantities they are putatively based on. I argue that the routine failure to consider the generalizability of one's conclusions from a statistical perspective lies at the root of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.">The Generalizability Crisis</span>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
						<td>
						<p>
							Statistical reproducibility
						</p>
			</td>
				</tr>
				<tr>
		<td>
			<p>
				<a href="http://proceedings.mlr.press/v97/bouthillier19a.html">Bouthillier et al <meta property="datePublished" content="2019-06-09">2019</a>
</p>
					</td>
			<td>
						<p>
<span title="The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.">Unreproducible Research is Reproducible</span>
</p>
					</td>
					<td>
						<p>
							Machine Learning
						</p>
					</td>
						<td>
						<p>
							Methodology
						</p>
			</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1038/s41567-019-0780-5">Milton & Possolo <meta property="datePublished" content="2019-06-09">2019</a>
	</td><td>
						<p><span title="Lack of reproducibility is not necessarily bad news; it may herald new discoveries and signal scientifc progress">Trustworthy data underpin reproducible research</span></p>
	</td>
	<td>
						<p>
							Physics
						</p>
					</td>
						<td>
						<p>
							Scientific philosophy
						</p>
			</td>
				</tr>
                                                                        <tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0216125"><span title="Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity">Devezer et al <meta property="datePublished" content="2019-05-15">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Consistent confirmations obtained independently of each other lend credibility to a scientific result. We refer to results satisfying this consistency as reproducible and assume that reproducibility is a desirable property of scientific discovery. Yet seemingly science also progresses despite irreproducible results, indicating that the relationship between reproducibility and other desirable properties of scientific discovery is not well understood. These properties include early discovery of truth, persistence on truth once it is discovered, and time spent on truth in a long-term scientific inquiry. We build a mathematical model of scientific discovery that presents a viable framework to study its desirable properties including reproducibility. In this framework, we assume that scientists adopt a model-centric approach to discover the true model generating data in a stochastic process of scientific discovery. We analyze the properties of this process using Markov chain theory, Monte Carlo methods, and agent-based modeling. We show that the scientific process may not converge to truth even if scientific results are reproducible and that irreproducible results do not necessarily imply untrue results. The proportion of different research strategies represented in the scientific population, scientists’ choice of methodology, the complexity of truth, and the strength of signal contribute to this counter-intuitive finding. Important insights include that innovative research speeds up the discovery of scientific truth by facilitating the exploration of model space and epistemic diversity optimizes across desirable properties of scientific discovery.">Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Statistical reproducibility
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2002.11626"><span title="A Realistic Guide to Making Data Available Alongside Code to Improve
  Reproducibility">Tierney et al <meta property="datePublished" content="2020-02-06">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Data makes science possible. Sharing data improves visibility, and makes the
research process transparent. This increases trust in the work, and allows for
independent reproduction of results. However, a large proportion of data from
published research is often only available to the original authors. Despite the
obvious benefits of sharing data, and scientists' advocating for the importance
of sharing data, most advice on sharing data discusses its broader benefits,
rather than the practical considerations of sharing. This paper provides
practical, actionable advice on how to actually share data alongside research.
The key message is sharing data falls on a continuum, and entering it should
come with minimal barriers.">A Realistic Guide to Making Data Available Alongside Code to Improve
  Reproducibility</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2003.00898"><span title="The importance of transparency and reproducibility in artificial
  intelligence research">Haibe-Kains et al <meta property="datePublished" content="2020-02-28">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="In their study, McKinney et al. showed the high potential of artificial
intelligence for breast cancer screening. However, the lack of detailed methods
and computer code undermines its scientific value. We identify obstacles
hindering transparent and reproducible AI research as faced by McKinney et al
and provide solutions with implications for the broader field.">The importance of transparency and reproducibility in artificial
  intelligence research</span>
						</p>
					</td>
					<td>
						<p>
							Artificial Intelligence
						</p>
					</td>
					<td>
						<p>
							Critique
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.3000691"><span title="What is replication?">Nosek & Errington <meta property="datePublished" content="2020-03-27">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="None">What is replication?</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Scientific philosophy
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.20944/preprints202006.0002.v1"><span title="Realistic and Robust Reproducible Research for Biostatistics">Hejblum et al <meta property="datePublished" content="2020-06-03">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="&lt;jats:p&gt;The complexity of analysis pipelines in biomedical sciences poses a severe challenge for the transparency and reproducibility of results. Researchers are increasingly incorporating software development technologies and methods into their analyses, but this is a quickly evolving landscape and teams may lack the capabilities to set up their own complex IT infrastructure to aid reproducibility. Basing a reproducible research strategy on readily available solutions with zero or low set-up costs whilst maintaining technological flexibility to incorporate domain-specific software tools is therefore of key importance. We outline a practical approach for robust reproducibility of analysis results. In our examples, we rely exclusively on established open-source tools and free services. Special emphasis is put on the integration of these tools with best practices from software development and free online services for the biostatistics domain.&lt;/jats:p&gt;">Realistic and Robust Reproducible Research for Biostatistics</span>
						</p>
					</td>
					<td>
						<p>
							Biostatistics
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/s41597-020-0524-5"><span title="COVID-19 pandemic reveals the peril of ignoring metadata standards">Schriml et al <meta property="datePublished" content="2020-06-19">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Abstract">COVID-19 pandemic reveals the peril of ignoring metadata standards</span>
						</p>
					</td>
					<td>
						<p>
							Virology
						</p>
					</td>
					<td>
						<p>
							Critique
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2007.08708"><span title="Principles for data analysis workflows">Stoudt et al <meta property="datePublished" content="2020-07-17">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Traditional data science education often omits training on research
workflows: the process that moves a scientific investigation from raw data to
coherent research question to insightful contribution. In this paper, we
elaborate basic principles of a reproducible data analysis workflow by defining
three phases: the Exploratory, Refinement, and Polishing Phases. Each workflow
phase is roughly centered around the audience to whom research decisions,
methodologies, and results are being immediately communicated. Importantly,
each phase can also give rise to a number of research products beyond
traditional academic publications. Where relevant, we draw analogies between
principles for data-intensive research workflows and established practice in
software development. The guidance provided here is not intended to be a strict
rulebook; rather, the suggestions for practices and tools to advance
reproducible, sound data-intensive analysis may furnish support for both
students and current professionals.">Principles for data analysis workflows</span>
						</p>
					</td>
					<td>
						<p>
							Data science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<!--theory_placeholder-->
	</tbody>
</table>

## Tool reviews
<table>
	<tbody>
		<tr>
			<td>
				<p>
					Authors/Date
				</p>
			</td>
			<td>
				<p>
					Title
				</p>
			</td>
			<td>
				<p>
					Tools
				</p>
			</td>
		</tr>
		<tr>
			<td>
				<p>
					<a href="https://www.researchgate.net/publication/337146640_Out-of-the-box_Reproducibility_A_Survey_of_Machine_Learning_Platforms">Isdahl & Gundersen <meta property="datePublished" content="2019-09-01">2019</a>
				</p>
			</td>
			<td>
				<p>
					<span title="Even machine learning experiments that are fully conducted on computers are not necessarily reproducible. An increasing number of open source and commercial, closed source machine learning platforms are being developed that help address this problem. However, there is no standard for assessing and comparing which features are required to fully support repro-ducibility. We propose a quantitative method that alleviates this problem. Based on the proposed method we assess and compare the current state of the art machine learning platforms for how well they support making empirical results reproducible. Our results show that BEAT and Floydhub have the best support for reproducibility with Codalab and Kaggle as close contenders. The most commonly used machine learning platforms provided by the big tech companies have poor support for reproducibility.">Out-of-the-box Reproducibility: A Survey of Machine Learning Platforms</span>
				</p>
			<td>
				<p>
					MLflow, Polyaxon, StudioML, Kubeflow, CometML, Sagemaker, GCPML, AzureML, Floydhub, BEAT, Codalab, Kaggle
				</p>
			</td>	
		</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1145/3311955">Pimentel et al
							<meta property="datePublished" content="2019-06-19">2019</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.">A Survey on Collecting, Managing, and Analyzing Provenance from Scripts</span>
						</p>
					</td>
					<td>
						<p>
							Astro-Wise, CPL, CXXR, Datatrack, ES3, ESSW, IncPy, Lancet, Magni, noWorkflow, Provenance Curios, pypet, RDataTracker, Sacred, SisGExp, SPADE, StarFlow, Sumatra, Variolite, VCR, versuchung, WISE, YesWorkflow
						</p>
					</td>
				</tr>
                                    		</tr>
                                    				<tr>
                                    					<td>
                                    						<p>
                                    							<a href="https://arxiv.org/abs/2006.08589">Leipzig et al
                                    							<meta property="datePublished" content="2020-06-15">2019</a> (<a href="https://github.com/leipzig/metadata-in-rcr">supplemental</a>)
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							<span title="Reproducible computational research (RCR) is the keystone of the scientific method for in silico analyses, packaging the transformation of raw data to published results. In addition to its role in research integrity, RCR has the capacity to significantly accelerate evaluation and reuse. This potential and wide-support for the FAIR principles have motivated interest in metadata standards supporting RCR. Metadata provides context and provenance to raw data and methods and is essential to both discovery and validation. Despite this shared connection with scientific data, few studies have explicitly described the relationship between metadata and RCR. This article employs a functional content analysis to identify metadata standards that support RCR functions across an analytic stack consisting of input data, tools, notebooks, pipelines, and publications. Our article provides background context, explores gaps, and discovers component trends of embeddedness and methodology weight from which we derive recommendations for future work.">The Role of Metadata in Reproducible Computational Research</span>
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							CellML, CIF2, DATS, DICOM, EML, FAANG, GBIF, GO, ISO/TC 276, MIAME, NetCDF, OGC, ThermoML, CRAN, Conda, pip setup.cfg, EDAM, CodeMeta, Biotoolsxsd, DOAP, ontosoft, SWO, OBCS, STATO, SDMX, DDI, MEX, MLSchema, MLFlow, Rmd, CWL, CWLProv, RO-Crate, RO, WICUS, OPM, PROV-O, ReproZip, ProvOne, WES, BagIt, BCO, ERC, BEL, DC, JATS, ONIX, MeSH, LCSH, MP, Open PHACTS, SWAN, SPAR, PWO, PAV, Manubot, ReScience, PandocScholar
                                    						</p>
                                    					</td>
                                    				</tr>
								                                    				<tr>
                                    					<td>
                                    						<p>
                                    							<a href="https://doi.org/10.1186/s41073-020-00095-y">Konkol, Markus, Nüst, Daniel, Goulier, Laura
                                    							<meta property="datePublished" content="2020-06-15">2020</a> 
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							<span title="The trend toward open science increases the pressure on authors to provide access to the source code and data they used to compute the results reported in their scientific papers. Since sharing materials reproducibly is challenging, several projects have developed solutions to support the release of executable analyses alongside articles. We reviewed 11 applications that can assist researchers in adhering to reproducibility principles. The applications were found through a literature search and interactions with the reproducible research community. An application was included in our analysis if it (i) was actively maintained at the time the data for this paper was collected, (ii) supports the publication of executable code and data, (iii) is connected to the scholarly publication process. By investigating the software documentation and published articles, we compared the applications across 19 criteria, such as deployment options and features that support authors in creating and readers in studying executable papers. From the 11 applications, eight allow publishers to self-host the system for free, whereas three provide paid services. Authors can submit an executable analysis using Jupyter Notebooks or R Markdown documents (10 applications support these formats). All approaches provide features to assist readers in studying the materials, e.g., one-click reproducible results or tools for manipulating the analysis parameters. Six applications allow for modifying materials after publication.">Publishing computational research - a review of infrastructures for reproducible and transparent scholarly communication</span>
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							Authorea, Binder, CodeOcean, eLife RDS, Galaxy Project, Gigantum, Manuscript, o2r, REANA, ReproZip, Whole tale
                                    						</p>
                                    					</td>
                                    				</tr>
				<!--tools_placeholder-->
	</tbody>
</table>

## Courses
- MOOCs
    - [Coursera Reproducible Research](https://www.coursera.org/learn/reproducible-research) - Roger Peng et al JHU. Very popular course.
    - [edX Principles, Statistical and Computational Tools for Reproducible Science](https://www.edx.org/course/principles-statistical-computational-harvardx-ph527x) - John Quackenbush et al Harvard
- Online course content
    - [Tools for Reproducible Research](http://kbroman.org/Tools4RR/) - Karl Broman UW, includes resources page
    - [R for Reproducible Scientific Analysis](https://swcarpentry.github.io/r-novice-gapminder/) - Software Carpentry workshop primer using Gapminder data
    - [R-DAVIS](https://gge-ucd.github.io/R-DAVIS/syllabus.html) - Student-developed computer literacy and data course in R
    - [AMIA2019](https://github.com/StatTag/amia-2019-spring-rr/) - Pragmatic RR for Analysis, Dissemination and Publication
    - [PSU-PSY525](https://github.com/psu-psychology/psy-525-reproducible-research-2020) - Transparent, Open, and Reproducible Research Practices in the Social and Behavioral Sciences
    - [OSU-OSRR](https://github.com/cbahlai/OSRR_course) - An open science and reproducible research course targeted at organismal ecologists

## Development Resources
- R
    - [CRAN Task View - Reproducible Research](https://cran.r-project.org/web/views/ReproducibleResearch.html) - packages relevant to RCR in R
    - [liftr](https://liftr.me/) - persistent reproducible reporting through containerized R Markdown documents
    - [repo](https://github.com/franapoli/repo) - provenance framework package
    - [orderly](https://vimc.github.io/orderly/articles/orderly.html) - R package that automates writing reproducible analyses

## User tools
- Open With Binder for [Chrome](https://matthiasbussonnier.com/posts/32-open-with-binder-chrome.html) or [Firefox](https://addons.mozilla.org/en-US/firefox/addon/open-with-binder/) - open the GitHub repository you are visiting using MyBinder.org
- [DVC](https://dvc.org/) - DVC tracks machine learning models and data sets
- [SciScore](https://www.sciscore.com/) - SciScore methods sections for a variety of rigor criteria and analyzes sentences that contain research resources (antibodies, cell lines, plasmids and software tools) and determines how uniquely identifiable that resource is based off of the provided metadata.
- [Ripeta](https://www.ripeta.com/) - Ripeta quickly scans research manuscripts or articles to identify and record key reproducibility variables, such as data availability, code acknowledgements, and research analysis methods.

## Books
- [Reproducible Research with R and R Studio 2013](https://g.co/kgs/RxcFNm)
- [Implementing Reproducible Research 2014](https://osf.io/s9tya/) - Describes projects: Sumatra, Vistrails, CDE, SOLE, JUMBO, CML, knitr. Content available on OSF.
- [The Practice of Reproducible Research 2017](https://g.co/kgs/jZiMR7) - 31 first person case narratives and intro chapters
- [Dynamic Documents with R and knitr 2015](https://g.co/kgs/dpzkF4)
- [The Turing Way: A Handbook for Reproducible Data Science 2020](https://the-turing-way.netlify.com/introduction/introduction)

## Databases
- [ReplicationWiki](http://replication.uni-goettingen.de/wiki/index.php) - Database for empirical studies with information about methods, data and software used, availability of replication material and whether replications, corrections or retractions are known. Mostly focused on social sciences.

## Data Repositories
All these repositories assign Digital Object Identifiers (DOIs) to data
- [DataCite](https://datacite.org) - 12M+ DOIs registered for 46 allocators. Offers APIs and a metadata schema.
- [Data Dryad](https://datadryad.org) - curated, metadata-centric, focused on articles associated with published artices, $120 submission fee (various waivers available)
- [Figshare](https://figshare.com) - 20 GB of free private space, unlimited public space, >2M articles, >5k projects
- [OSF](https://osf.io) - Project-oriented system with access control and integration with popular tools. Unlimited storage for projects, but individual files are limited to 5 gigabytes (GB) each.
- [Zenodo](https://zenodo.org/) - Allows embargoed, restricted access, metadata support. 50GB limit.

## Examples and Exemplars
- [Jupyter Gallery](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks) - Gallery of interesting Jupyter notebooks
- [Papers With Code](https://paperswithcode.com/) - ML papers with code
- [NARPS](https://github.com/poldrack/narps) - Code related to Neuroimaging Analysis Replication and Prediction Study
- [Codeocean](https://codeocean.com/explore) - A gallery of cloud-based containers with reproducible analyses

### Haibe-Kains lab reproducible papers
<table>
    <tr>
        <th>
            Publication
        </th>
        <th>
            CodeOcean link
        </th>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/471227">Mer AS et al. Integrative Pharmacogenomics Analysis of Patient Derived Xenografts</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/0566399">codeocean.com/capsule/056639</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/052910">Gendoo, Zon et al. MetaGxData: Clinically Annotated Breast, Ovarian and Pancreatic Cancer Datasets and their Use in Generating a Multi-Cancer Gene Signature</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/6438633">codeocean.com/capsule/643863</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1093/jamia/ocx062">Yao et al. Tissue specificity of in vitro drug sensitivity</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/5502756">codeocean.com/capsule/550275</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1038/s41467-017-02136-5">Safikhani Z et al. Gene isoforms as expression-based biomarkers predictive of drug response in vitro</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/0002901">codeocean.com/capsule/000290</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1158/0008-5472.CAN-17-0096">El-Hachem et al. Integrative cancer pharmacogenomics to infer large-scale drug taxonomy</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/4252248">codeocean.com/capsule/425224</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.12688/f1000research.9611.3">Safikhani Z et al. Revisiting inconsistency in large pharmacogenomic studies</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/6276064">codeocean.com/capsule/627606</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/355602">Sandhu V et al. Meta-analysis of 1,200 transcriptomic profiles identifies a prognostic model for pancreatic ductal adenocarcinoma</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/2693620">codeocean.com/capsule/269362</a>
        </td>
    </tr>
</table>


## Journals
- [ReScience](http://rescience.github.io/) - Journal dedicated to insilico reproductions and tests of robustness, lives on Github.

## Ontologies
- [FAIRsharing](https://fairsharing.org) - standards, databases, and policies
- [BioPortal](https://bioportal.bioontology.org/) - 660 biomedical ontologies 

## Organizations
- [ResearchObject.org](http://www.researchobject.org/) - RO specifications and publications
- [BioCompute](https://osf.io/zm97b/) - BCO specs
- [rOpenSci](https://ropensci.org) - Tools, conferences, and education
- [Open Science Framework](https://osf.io) - Open source project management
- [pyOpenSci](https://www.pyopensci.org/) - Promotes open and reproducible research through peer-review of scientific Python packages
- [Replication Network](https://replicationnetwork.com/) - Furthering the practice of replication in economics. Econ replication database.
- [repliCATS project](https://replicats.research.unimelb.edu.au/) - Estimating the replicability of research in the social sciences
- [ReproHack](https://reprohack.github.io/reprohack-hq/) - 1-day reproducibility hackathons held worldwide
- [CODECHECK](https://codecheck.org.uk/) - community for checking executability of scientific preprints and papers

## Awesome Lists
- [Awesome Pipeline](https://github.com/pditommaso/awesome-pipeline) - So many pipelines frameworks
- [Awesome Docker](https://github.com/veggiemonk/awesome-docker) - Everything related to the Docker containerization system
- [Awesome R](https://github.com/qinwf/awesome-R#reproducible-research) - Section on RR tools
- [Awesome Reproducible R](https://github.com/datasnakes/awesome-reproducible-R) - RRR tools
- [Awesome Jupyter](https://github.com/adebar/awesome-jupyter) - Jupyter projects, libraries and resources
- [Awesome Bioinformatics Benchmarks](https://github.com/j-andrews7/Awesome-Bioinformatics-Benchmarks) - Benchmarks are a related aspect of robustness testing
- [Awesome Open Science](https://github.com/ZoranPandovski/awesome-open-science) - Resources, data, tools, and scholarship
- [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets) - A topic-centric list of HQ open datasets
- [Awesome Semantic Web](https://github.com/semantalytics/awesome-semantic-web) - Semantic web and linked data resources.

## Contribute

Contributions welcome! Read the [contribution guidelines](contributing.md) first. You may find my `src/doi2md.py` script useful for quickly generating entries from a DOI.


## License

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, Jeremy Leipzig has waived all copyright and
related or neighboring rights to this work.
